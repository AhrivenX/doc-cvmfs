\chapter{\cvmfs\ Server Infrastructure}
\label{apx:serverinfrastructure}

This is trying to be a full documentation of a \cvmfs\ server setup including the infrastructure necessary for an individual repository.
It is highly recommended to first consult Section~\ref{sct:repoanatomy} for a more general overview of the involved directory structure.

\section{Prerequisites}
A \cvmfs\ server installation depends on some environment setup and tools to be in place:
\begin{itemize}
\item \aufs\ support in the kernel (see Section~\ref{sct:customkernelinstall})
\item Backend storage location available through HTTP
\item Backend storage accessible at \texttt{/srv/cvmfs/...} (unless stored on S3)
\item \textbf{cvmfs} and \textbf{cvmfs-server} packages installed
\end{itemize}
\pagebreak

\section{Local Backend Storage Infrastructure}
\cvmfs\ stores the entire repository content (file content and meta-data catalogs) into a content addressable storage (CAS).
This storage can either be a file system at \texttt{/srv/cvmfs} or an S3 compatible key-value storage system (see Section~\ref{sct:s3storagesetup} for details).
In the former case the contents of \texttt{/srv/cvmfs} are as follows:

\LTXtable{\textwidth}{figures/tablocalstorageanatomy.tex}
\pagebreak

\section{Server Spool Area of a Repository (Stratum0)}
The spool area of a repository contains transaction infrastructure and scratch area of a Stratum0 or specifically a release manager machine installation.
It is always located inside \texttt{/var/spool/cvmfs} with directories for individual repositories.
Note that the data volume of the spool area can grow very large for massive repository updates since it contains the writable AUFS branch and a \cvmfs\ client cache directory.

\LTXtable{\textwidth}{figures/tabrepospoolanatomy.tex}
\pagebreak

\section{Repository Configuration Directory}
The authoritative configuration of a \cvmfs\ repository is located in \linebreak
\texttt{/etc/cvmfs/repositories.d} and should only be writable by the administrator.
Furthermore the repository's keychain is located in \texttt{/etc/cvmfs/keys} and follows the naming convention \texttt{<fqrn>.crt} for the certificate, \texttt{<fqrn>.key} for the repository's private key and \texttt{<fqrn>.pub} for the public key.
All of those files can be symlinked somewhere else if necessary.

\LTXtable{\textwidth}{figures/tabrepoconfiganatomy.tex}
\pagebreak

\section{Environment Setup}
Apart from file and directory locations a \cvmfs\ server installation depends on a few environment configurations.
Most notably the possibility to access the backend storage through HTTP and to allow for mounting of both the \cvmfs\ client at \texttt{/var/spool/cvmfs/<fqrn>/rdonly} and \aufs\ on \texttt{/cvmfs/<fqrn>}.

Granting HTTP access can happen in various ways and depends on the chosen backend storage type.
For an S3 hosted backend storage, the \cvmfs\ client can usually be directly pointed to the S3 bucket used for storage (see Section~\ref{sct:s3storagesetup} for details).
In case of a local file system backend any web server can be used for this purpose.
By default \cvmfs\ assumes Apache and uses that automatically.

Internally the \cvmfs\ server uses a SUID binary (i.e. \texttt{cvmfs\_suid\_helper}) to manipulate its mount points.
This is necessary, since transactional \cvmfs\ commands must be accessible to the repository owner that is usually different from root.
Both the mount directives for \texttt{/var/spool/cvmfs/<fqrn>/rdonly} and \texttt{/cvmfs/<fqrn>} must be placed into \texttt{/etc/fstab} for this reason.
By default \cvmfs\ uses the following entries for these mount points:

\begin{verbatim}
cvmfs2#<fqrn> /var/spool/cvmfs/<fqrn>/rdonly fuse \
allow_other,config=/etc/cvmfs/repositories.d/<fqrn>/client.conf: \
/var/spool/cvmfs/<fqrn>/client.local,cvmfs_suid 0 0

aufs_<fqrn> /cvmfs/<fqrn> aufs br=/var/spool/cvmfs/<fqrn>/scratch=rw: \
/var/spool/cvmfs/<fqrn>/rdonly=rr,udba=none,ro 0 0
\end{verbatim}
