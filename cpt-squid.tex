\chapter{Setting up a Local Squid Proxy}
\label{sct:squid}

If you setup \cvmfs\ on your local cluster, we strongly recommend to setup a Squid forward proxy server as well.
This is for two reasons: it will reduce the latency for the local worker nodes, which is critical for cold cache performance. 
And it reduces the load on our backend server.

From what we have seen, a Squid server on commodity hardware scales well for at least a couple of hundred worker nodes.
The more RAM and hard disk you can devote for caching the better.
We have good experience with 4-8\,GB of memory cache and 50-100\,GB of hard disk cache.
We suggest to setup two identical Squids for reliability and load-balancing.
Assuming the two servers are A and B, adjust your \cvmfs\ client configuration as follows:
\begin{lstlisting}[language=bash]
CVMFS_HTTP_PROXY="http://A:3128|http://B:3128"
\end{lstlisting}

Squid is very powerful and has lots of configuration and tuning options.
For \cvmfs\ we require only the very basic static content caching.
Starting from a standard Scientific Linux 5 Squid, all we have to do is to adjust the cache size.
If you're using ACLs, add ACLs allow rules for the Stratum 1 servers\footnote{For Cern repositories Stratum 1 servers are \url{http://cvmfs-stratum-one.cern.ch:8000} (CERN), \url{http://cernvmfs.gridpp.rl.ac.uk:8000} (RAL), and \url{http://cvmfs.racf.bnl.gov:8000} (BNL).}.
Browse through your /etc/squid/squid.conf and make sure the following lines appear accordingly
\begin{lstlisting}[language=bash]
collapsed_forwarding on
max_filedesc 8192
maximum_object_size 4096 MB

# 4 GB memory cache
cache_mem 4096 MB
maximum_object_size_in_memory 128 KB
# 50 GB disk cache
cache_dir ufs /var/spool/squid 50000 16 256
\end{lstlisting}

Check your Squid configuration with \texttt{squid -k parse}.
Create the hard disk cache area with \texttt{squid -z}.
In order to make the increased number of file descriptors effective for Squid, execute \texttt{ulimit -n 8192} prior to starting the Squid service.
