\chapter{Setting up a Replica Server (Stratum~1)}
\label{sct:replica}

While a \cvmfs\ Stratum 0 repository server is able to serve clients directly, a large number of clients is better be served by a set of Stratum~1 replica servers.
Multiple Stratum~1 servers improve the reliability, reduce the load, and protect the Stratum 0 master copy of the repository from direct accesses.
Stratum~0 server, Stratum~1 servers and the site-local proxy servers can be seen as content distribution network.
Figure~\ref{fig:stratum1} shows the situation for the repositories hosted in the cern.ch domain.

\begin{figure}
	\begin{center}
		\resizebox{\textwidth}{!}{\input{figures/stratum1}}
	\end{center}
	\caption{\cvmfs\ content distribution network for the cern.ch domain: Stratum~1 replica servers are located in Europe, the U.S.\ and Asia.  
		One protected read/write instance (Stratum 0) is feeding up the public, distributed mirror servers. 
		A distributed hierarchy of proxy servers fetches content from the closest public mirror server.}
	\label{fig:stratum1}
\end{figure}

A Stratum~1 server is a standard web server that uses the \cvmfs\ server toolkit to create and maintain a mirror of a \cvmfs\ repository served by a Stratum~0 server.
To this end, the \texttt{cvmfs\_server} utility provides the \texttt{add-replica} command.
This command will register the Stratum~0 URL and prepare the local web server.
Periodical synchronization has to be scheduled, for instance with \texttt{cron}, using the \texttt{cvmfs\_server snapshot} command.
The advantage over general purpose mirroring tools such as \product{rSync} is that all \cvmfs\ file integrity verifications mechanisms from the \fuse\ client are reused.
Additionally, by the aid of the \cvmfs\ file catalogs, the \texttt{cvmfs\_server} utility knows beforehand (without remote listing) which files to transfer.

In order to prevent accidental synchronization from a repository, the Stratum~0 repository maintainer has to create a \texttt{.cvmfs\_master\_replica} file in the HTTP root directory.
This file is created by default when a new repository is created.
Note that replication can thrash caches that might exist between Stratum~1 and Stratum~0.
A direct connection is therefore preferable.

\section{Recommended Setup}
The vast majority of HTTP requests will be served by the site's local
proxy servers.
Being a publicly available service, however, we recommend to install
a \squid\ frontend in front of the Stratum~1 web server.

We suggest the following key parameters:
\begin{description}
	\item[Storage.] 
		RAID-protected storage.
		The \texttt{cvmfs\_server} utility should have low latency to the storage because it runs a large number of system calls (\texttt{stat()}) against it.
		For the local storage backends ext3/4 filesystems are preferred (rather than XFS).
	\item[Web server.]
		A standard Apache server.
		Directory listing is not required.
		In addition, it is a good practice to exclude search engines from the replica web server by an appropriate robots.txt.
		The webserver should be close to the storage in terms of latency.
	\item[Squid frontend.]
		\squid\ should be used as a frontend to Apache, configured
		as a reverse proxy.  It is recommended to run it on the
		same machine as Apache to reduce the number of points of
		failure.  Alternatively, separate \squid\ server machines
		may be configured in load-balance mode forwarding to the
		Apache server, but note that if any of them are down
		the entire service will be considered down by \cvmfs\
		clients.
		The Squid frontend should listen on ports 80 and 8000.
		The more RAM that the operating system can use for file
		system caching, the better.

	\item[DNS cache.]
		A Stratum 1 does a lot of DNS lookups, so we recommend
		installing a DNS caching mechanism on the machine such
		as \texttt{dnsmasq} or \texttt{bind}.  We do not recommend
		\texttt{nscd} since it does not honor the DNS Time-To-Live
		protocol.
\end{description}

\section{Squid Configuration}
The \squid\ configuration differs from the site-local Squids
because the Stratum~1 \squid\ servers are transparent to the clients
(\emph{reverse proxy}).
As the expiry rules are set by the web server, \squid\ cache expiry
rules remain unchanged.

The following lines should appear accordingly in /etc/squid/squid.conf:
\begin{verbatim}
  http_port 80 accel
  http_port 8000 accel
  http_access allow all
  cache_peer <APACHE_HOSTNAME> parent <APACHE_PORT> 0 no-query originserver

  cache_mem <MEM_CACHE_SIZE> MB
  cache_dir ufs /var/spool/squid <DISK_CACHE_SIZE in MB> 16 256
  maximum_object_size 1024 MB
  maximum_object_size_in_memory 128 KB
\end{verbatim}
\quad\\
Note that \texttt{http\_access allow all} has to be inserted before (or instead of) the line \texttt{http\_access deny all}.
If Apache is running on the same host, the \texttt{APACHE\_HOSTNAME}
will be \texttt{localhost}.  Also, in that case there is not a
performance advantage for squid to cache files that came from the same
machine, so you can configure squid to not cache files.  Do that with
the following lines:
\begin{verbatim}
  acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/
  cache deny !CVMFSAPI
\end{verbatim}
Then the squid will only cache API calls.  You can then set
\texttt{MEM\_CACHE\_SIZE} and \texttt{DISK\_CACHE\_SIZE} quite small.

Check the configuration syntax by \texttt{squid -k parse}.
Create the hard disk cache area with \texttt{squid -z}. 
In order to make the increased number of file descriptors effective for \squid, execute \texttt{ulimit -n 8192} prior to starting the squid service.

\section{Monitoring}
The \texttt{cvmfs\_server} utility reports status and problems to \texttt{stdout} and \texttt{stderr}.

For the web server infrastructure, we recommend standard \nagios\ HTTP checks.
They should be configured with the URL \url{http://$replica-server/cvmfs/$repository_name/.cvmfspublished}.
This file can also be used to monitor if the same repository revision is served by the Stratum~0 server and all the Stratum~1 servers.
In order to tune the hardware and cache sizes, keep an eye on the Squid server's CPU and I/O load.

Keep an eye on HTTP 404 errors.
For normal CernVM-FS traffic, such failures should not occur.
Traffic from \cvmfs\ clients is marked by an \texttt{X-CVMFS2} header.
